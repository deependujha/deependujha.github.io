---
title: Distributed Training
description: "Distributed training is a technique used to accelerate the training of machine learning models by leveraging multiple computing resources, such as GPUs or TPUs, across multiple machines or nodes."
date: 2025-06-26
authors:
  - name: Deependu
    link: https://github.com/deependujha
    image: https://github.com/deependujha.png
tags:
  - PyTorch
  - Distributed Training
  - Deep Learning
math: true
weight: 98
---

## Roadmap

- Distributed Data Parallel (DDP)
- FSDP (Fully Sharded Data Parallel)
- Tensor Parallelism (TP)
- Pipeline Parallelism
- Device Mesh (Dtensor & DeviceMesh)
- Remote Procedure Call (RPC) distributed training
- Custom Extensions
